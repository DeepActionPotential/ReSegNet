{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6318833,"sourceType":"datasetVersion","datasetId":3636171}],"dockerImageVersionId":30528,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n## **1. Problem Definition**\n\n* **Identify the Task**: Define the vision task—e.g., image classification, object detection, or segmentation.\n* **Determine the Goal**: Establish what success looks like—e.g., target accuracy, latency, or generalization.\n* **Specify Constraints**: Consider hardware limitations, deployment environment, and dataset availability.\n* **Use Case Examples**: Medical imaging, autonomous vehicles, retail analytics, etc.\n\n---\n\n### **2. Data Preparation**\n\n* Dataset loading, preprocessing, augmentation, and train/validation/test splitting.\n\n---\n\n### **3. Choose or Define Model**\n\n* Select pretrained architectures or design a custom model for your task.\n\n---\n\n### **4. Define Loss Function and Optimizer**\n\n* Match loss functions and optimizers to the problem type (classification, detection, etc.).\n\n---\n\n### **5. Train the Model**\n\n* Setup of training loops, optimization steps, and progress monitoring.\n\n---\n\n### **6. Evaluate the Model**\n\n* Test set performance metrics and visualizations.\n\n---\n\n### **7. Save**\n\n* Model serialization, export to deployment-friendly formats, and integration into applications.\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"\n## **1. Problem Definition: Retina Blood Vessel Segmentation**\n\nThe goal is to develop a deep learning model that segments blood vessels from retinal fundus images using the [Retina Blood Vessel dataset](https://www.kaggle.com/datasets/abdallahwagih/retina-blood-vessel/data). Accurate segmentation of retinal vessels is a critical step in diagnosing and monitoring eye diseases such as diabetic retinopathy, glaucoma, and hypertensive retinopathy.\n\nThe dataset provides color fundus images along with corresponding ground truth masks highlighting the vascular structure. This is a pixel-wise binary classification task, where the model must distinguish vessel pixels from the background.\n\n### Key Challenges:\n\n* **Class Imbalance**: Blood vessels cover a small fraction of each image, making it easy for the model to be biased toward predicting background.\n* **Fine Structural Detail**: Vessels are thin, branching, and vary in intensity, requiring high-resolution feature extraction and spatial precision.\n* **Image Variability**: Differences in illumination, contrast, and noise between samples increase the complexity of generalization.\n\n### Success Criteria:\n\n* High segmentation quality measured by **Dice coefficient**, **IoU**, **Precision**, and **Recall**.\n* Robust generalization to unseen data, especially across varying image qualities.\n* Efficient inference for potential integration in screening tools or clinical workflows.\n\n","metadata":{}},{"cell_type":"markdown","source":"#### Tools\n","metadata":{}},{"cell_type":"code","source":"!pip install segmentation-models-pytorch --quiet\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T15:25:12.940169Z","iopub.execute_input":"2025-05-03T15:25:12.940550Z","iopub.status.idle":"2025-05-03T15:25:22.959425Z","shell.execute_reply.started":"2025-05-03T15:25:12.940523Z","shell.execute_reply":"2025-05-03T15:25:22.958333Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Standard Library\nimport os\nimport time\nimport random\nfrom glob import glob\nfrom operator import add\nfrom pathlib import Path\n\n\n# Third-Party Libraries\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import (\n    accuracy_score,\n    f1_score,\n    jaccard_score,\n    precision_score,\n    recall_score\n)\n\n# PyTorch and Related Modules\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport segmentation_models_pytorch as smp\n\n\n","metadata":{"execution":{"iopub.status.busy":"2025-05-03T15:25:22.961196Z","iopub.execute_input":"2025-05-03T15:25:22.961513Z","iopub.status.idle":"2025-05-03T15:25:28.468576Z","shell.execute_reply.started":"2025-05-03T15:25:22.961488Z","shell.execute_reply":"2025-05-03T15:25:28.467881Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **2. Data Preparation**\n","metadata":{}},{"cell_type":"code","source":"class ImageMaskDataset(Dataset):\n    def __init__(self, image_paths, mask_paths):\n        self.image_paths = [Path(p) for p in image_paths]\n        self.mask_paths  = [Path(p) for p in mask_paths]\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img = self._load_image(self.image_paths[idx])\n        msk = self._load_mask(self.mask_paths[idx])\n        return img, msk\n\n    def _load_image(self, path: Path):\n        arr = cv2.imread(str(path), cv2.IMREAD_COLOR)\n        arr = arr.astype(np.float32) / 255.0\n        arr = np.transpose(arr, (2, 0, 1))\n        return torch.from_numpy(arr)\n\n    def _load_mask(self, path: Path):\n        arr = cv2.imread(str(path), cv2.IMREAD_GRAYSCALE)\n        arr = arr.astype(np.float32) / 255.0\n        arr = np.expand_dims(arr, 0)\n        return torch.from_numpy(arr)\n","metadata":{"execution":{"iopub.status.busy":"2025-05-03T15:25:28.469533Z","iopub.execute_input":"2025-05-03T15:25:28.469770Z","iopub.status.idle":"2025-05-03T15:25:28.476479Z","shell.execute_reply.started":"2025-05-03T15:25:28.469749Z","shell.execute_reply":"2025-05-03T15:25:28.475526Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ——— Configuration ———\nconfig = {\n    \"seed\": 42,\n    \"data_root\": Path(\"/kaggle/input/retina-blood-vessel/Data\"),\n    \"img_size\": (512, 512),\n    \"batch_size\": 2,\n    \"lr\": 1e-4,\n    \"checkpoint_dir\": Path(\"files\") / \"checkpoint.pth\",\n    \"epochs\": 50,\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T15:25:28.478204Z","iopub.execute_input":"2025-05-03T15:25:28.478476Z","iopub.status.idle":"2025-05-03T15:25:28.486957Z","shell.execute_reply.started":"2025-05-03T15:25:28.478455Z","shell.execute_reply":"2025-05-03T15:25:28.486282Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_dir(path):\n    \"\"\"\n    Ensure that a directory exists (creates it if necessary).\n    Accepts either a string or Path.\n    \"\"\"\n    Path(path).mkdir(parents=True, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T15:25:28.487957Z","iopub.execute_input":"2025-05-03T15:25:28.488503Z","iopub.status.idle":"2025-05-03T15:25:28.496632Z","shell.execute_reply.started":"2025-05-03T15:25:28.488472Z","shell.execute_reply":"2025-05-03T15:25:28.495891Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ——— Setup ———\ncreate_dir(config[\"checkpoint_dir\"].parent)\ncheckpoint_path = \"files/checkpoint.pth\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T15:25:28.497617Z","iopub.execute_input":"2025-05-03T15:25:28.497918Z","iopub.status.idle":"2025-05-03T15:25:28.505710Z","shell.execute_reply.started":"2025-05-03T15:25:28.497889Z","shell.execute_reply":"2025-05-03T15:25:28.505024Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ——— Helpers ———\ndef get_paths(root: Path, split: str, kind: str):\n    return sorted((root / split / kind).glob(\"*\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T15:25:28.506793Z","iopub.execute_input":"2025-05-03T15:25:28.507092Z","iopub.status.idle":"2025-05-03T15:25:28.518396Z","shell.execute_reply.started":"2025-05-03T15:25:28.507063Z","shell.execute_reply":"2025-05-03T15:25:28.517677Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ——— Data Paths ———\ntrain_x = get_paths(config[\"data_root\"], \"train\", \"image\")\ntrain_y = get_paths(config[\"data_root\"], \"train\", \"mask\")\nvalid_x = get_paths(config[\"data_root\"], \"test\",  \"image\")\nvalid_y = get_paths(config[\"data_root\"], \"test\",  \"mask\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T15:25:28.519453Z","iopub.execute_input":"2025-05-03T15:25:28.519766Z","iopub.status.idle":"2025-05-03T15:25:28.597651Z","shell.execute_reply.started":"2025-05-03T15:25:28.519729Z","shell.execute_reply":"2025-05-03T15:25:28.596991Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\n    f\"Dataset Size:\\n\"\n    f\"  Train: {len(train_x)} samples\\n\"\n    f\"  Valid: {len(valid_x)} samples\\n\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T15:25:28.598710Z","iopub.execute_input":"2025-05-03T15:25:28.599030Z","iopub.status.idle":"2025-05-03T15:25:28.603703Z","shell.execute_reply.started":"2025-05-03T15:25:28.599001Z","shell.execute_reply":"2025-05-03T15:25:28.602945Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ——— Datasets & Loaders ———\ntrain_dataset = ImageMaskDataset(train_x, train_y)\nvalid_dataset = ImageMaskDataset(valid_x, valid_y)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=config[\"batch_size\"],\n    shuffle=True,\n    num_workers=2,\n)\nvalid_loader = DataLoader(\n    valid_dataset,\n    batch_size=config[\"batch_size\"],\n    shuffle=False,\n    num_workers=2,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T15:25:28.606313Z","iopub.execute_input":"2025-05-03T15:25:28.606547Z","iopub.status.idle":"2025-05-03T15:25:28.618622Z","shell.execute_reply.started":"2025-05-03T15:25:28.606527Z","shell.execute_reply":"2025-05-03T15:25:28.617872Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset[0][0].shape, valid_dataset[0][0].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T15:25:28.619564Z","iopub.execute_input":"2025-05-03T15:25:28.619870Z","iopub.status.idle":"2025-05-03T15:25:28.710432Z","shell.execute_reply.started":"2025-05-03T15:25:28.619839Z","shell.execute_reply":"2025-05-03T15:25:28.709479Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **3. Choose or Define Model**\n","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T15:25:28.711562Z","iopub.execute_input":"2025-05-03T15:25:28.712236Z","iopub.status.idle":"2025-05-03T15:25:28.739724Z","shell.execute_reply.started":"2025-05-03T15:25:28.712183Z","shell.execute_reply":"2025-05-03T15:25:28.738665Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = smp.Unet(\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    in_channels=3,\n    classes=1,           \n    activation=None\n).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T15:25:28.740722Z","iopub.execute_input":"2025-05-03T15:25:28.740999Z","iopub.status.idle":"2025-05-03T15:25:31.026920Z","shell.execute_reply.started":"2025-05-03T15:25:28.740965Z","shell.execute_reply":"2025-05-03T15:25:31.025934Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **4. Define Loss Function and Optimizer**\n","metadata":{}},{"cell_type":"code","source":"bce_loss  = nn.BCEWithLogitsLoss()\ndice_loss = smp.losses.DiceLoss(mode=\"binary\")\n\n\ndef loss_fn(preds, targets):\n    return bce_loss(preds, targets) + dice_loss(preds, targets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T15:25:31.028050Z","iopub.execute_input":"2025-05-03T15:25:31.028342Z","iopub.status.idle":"2025-05-03T15:25:31.033001Z","shell.execute_reply.started":"2025-05-03T15:25:31.028318Z","shell.execute_reply":"2025-05-03T15:25:31.032112Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=config['lr'])\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode=\"min\", patience=3, factor=0.5\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T15:25:31.034053Z","iopub.execute_input":"2025-05-03T15:25:31.034333Z","iopub.status.idle":"2025-05-03T15:25:31.045882Z","shell.execute_reply.started":"2025-05-03T15:25:31.034311Z","shell.execute_reply":"2025-05-03T15:25:31.045001Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **5. Train the Model**\n","metadata":{}},{"cell_type":"code","source":"def train_one_epoch(loader):\n    model.train()\n    running_loss = 0.0\n    for images, masks in tqdm(loader, desc=\"Train\"):\n        images, masks = images.to(device), masks.to(device)\n        preds = model(images)\n        loss  = loss_fn(preds, masks)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    return running_loss / len(loader)\n\n\n\ndef validate(loader):\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for images, masks in tqdm(loader, desc=\"Validate\"):\n            images, masks = images.to(device), masks.to(device)\n            preds = model(images)\n            val_loss += loss_fn(preds, masks).item()\n    return val_loss / len(loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T15:25:31.047252Z","iopub.execute_input":"2025-05-03T15:25:31.047549Z","iopub.status.idle":"2025-05-03T15:25:31.057779Z","shell.execute_reply.started":"2025-05-03T15:25:31.047521Z","shell.execute_reply":"2025-05-03T15:25:31.057114Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_val = float(\"inf\")\nfor epoch in range(1, config['epochs']):\n    train_loss = train_one_epoch(train_loader)\n    val_loss   = validate(valid_loader)\n    scheduler.step(val_loss)\n\n    print(f\"Epoch {epoch:02d} — train: {train_loss:.4f}, val: {val_loss:.4f}\")\n    if val_loss < best_val:\n        best_val = val_loss\n        torch.save(model.state_dict(), checkpoint_path)\n        print(\"  → checkpoint saved\")\n\n# Inference Example\nmodel.load_state_dict(torch.load(checkpoint_path))\nmodel.eval()\nwith torch.no_grad():\n    img, _ = valid_dataset[0]\n    pred = model(img.unsqueeze(0).to(device))\n    mask = torch.sigmoid(pred).cpu().squeeze().numpy() > 0.5\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T15:25:31.058886Z","iopub.execute_input":"2025-05-03T15:25:31.059121Z","iopub.status.idle":"2025-05-03T15:28:30.163032Z","shell.execute_reply.started":"2025-05-03T15:25:31.059098Z","shell.execute_reply":"2025-05-03T15:28:30.162023Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"valid_dataset[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T15:28:30.164362Z","iopub.execute_input":"2025-05-03T15:28:30.165307Z","iopub.status.idle":"2025-05-03T15:28:30.202923Z","shell.execute_reply.started":"2025-05-03T15:28:30.165274Z","shell.execute_reply":"2025-05-03T15:28:30.202054Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **6. Evaluate the Model**\n","metadata":{}},{"cell_type":"code","source":"# Helpers -------------------------------------------------------------------\n\ndef ensure_dir_exists(path: str):\n    os.makedirs(path, exist_ok=True)\n\ndef tensor_to_numpy_image(tensor: torch.Tensor) -> np.ndarray:\n    arr = tensor.cpu().numpy().transpose(1, 2, 0)\n    return (arr * 255).astype(np.uint8)\n\ndef tensor_to_binary_mask(tensor: torch.Tensor, threshold: float = 0.5) -> np.ndarray:\n    arr = tensor.cpu().numpy().squeeze()\n    return (arr > threshold).astype(np.uint8)\n\ndef expand_mask_to_rgb(mask: np.ndarray) -> np.ndarray:\n    return np.stack([mask]*3, axis=-1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T15:28:30.203946Z","iopub.execute_input":"2025-05-03T15:28:30.204185Z","iopub.status.idle":"2025-05-03T15:28:30.210106Z","shell.execute_reply.started":"2025-05-03T15:28:30.204164Z","shell.execute_reply":"2025-05-03T15:28:30.209257Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Metrics -------------------------------------------------------------------\n\nfrom sklearn.metrics import (\n    jaccard_score, f1_score,\n    recall_score, precision_score,\n    accuracy_score\n)\n\ndef compute_metrics_for_sample(y_true: torch.Tensor, y_pred: torch.Tensor):\n    \"\"\"\n    Returns [jaccard, f1, recall, precision, accuracy] for a single sample.\n    \"\"\"\n    y_t = (y_true.cpu().numpy().ravel() > 0.5).astype(np.uint8)\n    y_p = (y_pred.cpu().numpy().ravel() > 0.5).astype(np.uint8)\n\n    return [\n        jaccard_score(y_t, y_p),\n        f1_score(y_t, y_p),\n        recall_score(y_t, y_p),\n        precision_score(y_t, y_p),\n        accuracy_score(y_t, y_p),\n    ]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T15:28:30.211193Z","iopub.execute_input":"2025-05-03T15:28:30.211498Z","iopub.status.idle":"2025-05-03T15:28:30.221165Z","shell.execute_reply.started":"2025-05-03T15:28:30.211475Z","shell.execute_reply":"2025-05-03T15:28:30.220265Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# I/O -----------------------------------------------------------------------\n\ndef save_comparison_image(\n    orig_img: np.ndarray,\n    gt_mask: np.ndarray,\n    pred_mask: np.ndarray,\n    save_dir: str,\n    filename: str,\n    img_size: tuple\n):\n    height, width = img_size\n    separator = np.ones((height, 10, 3), dtype=np.uint8) * 128\n\n    gt_rgb   = expand_mask_to_rgb(gt_mask)\n    pred_rgb = expand_mask_to_rgb(pred_mask)\n\n    composite = np.concatenate(\n        [orig_img, separator, gt_rgb, separator, pred_rgb],\n        axis=1\n    )\n    cv2.imwrite(os.path.join(save_dir, filename), composite)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T15:28:30.222133Z","iopub.execute_input":"2025-05-03T15:28:30.222388Z","iopub.status.idle":"2025-05-03T15:28:30.236284Z","shell.execute_reply.started":"2025-05-03T15:28:30.222366Z","shell.execute_reply":"2025-05-03T15:28:30.235490Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Main Evaluation -----------------------------------------------------------\n\ndef evaluate_model(\n    model: torch.nn.Module,\n    data_loader: torch.utils.data.DataLoader,\n    device: torch.device,\n    results_dir: str,\n    img_size: tuple\n):\n    ensure_dir_exists(results_dir)\n    model.to(device).eval()\n\n    total_metrics = np.zeros(5, dtype=float)\n    n_samples = len(data_loader.dataset)\n    sample_idx = 0\n\n    with torch.no_grad():\n        for imgs, masks in tqdm(data_loader, desc=\"Evaluating\", total=len(data_loader)):\n            imgs  = imgs.to(device)\n            masks = masks.to(device)\n\n            preds = torch.sigmoid(model(imgs))\n\n            for img_t, mask_t, pred_t in zip(imgs, masks, preds):\n                metrics = compute_metrics_for_sample(mask_t, pred_t)\n                total_metrics += np.array(metrics)\n\n                orig     = tensor_to_numpy_image(img_t)\n                gt       = tensor_to_binary_mask(mask_t) * 255\n                pr       = tensor_to_binary_mask(pred_t) * 255\n                filename = f\"sample_{sample_idx:04d}.png\"\n                save_comparison_image(orig, gt, pr, results_dir, filename, img_size)\n                sample_idx += 1\n\n    avg_metrics = total_metrics / n_samples\n    jaccard, f1, recall, precision, accuracy = avg_metrics\n    \n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n    print(f\"Recall:   {recall:.4f}\")\n    print(f\"Precision:{precision:.4f}\")\n    print(f\"Jaccard:  {jaccard:.4f}\")\n\n# Usage ---------------------------------------------------------------------\nmodel.load_state_dict(torch.load(checkpoint_path))\nevaluate_model(model, valid_loader, device, \"results\", config[\"img_size\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T15:28:30.237372Z","iopub.execute_input":"2025-05-03T15:28:30.237623Z","iopub.status.idle":"2025-05-03T15:28:36.198983Z","shell.execute_reply.started":"2025-05-03T15:28:30.237602Z","shell.execute_reply":"2025-05-03T15:28:36.197783Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()\n\n# Number of examples to display\nnum_examples = 10\n\nfor idx in range(num_examples):\n    #  Get image & mask from your dataset\n    img_t, mask_t = valid_dataset[idx]                # img_t: Tensor [3,H,W], mask_t: Tensor [1,H,W]\n    \n    #  Run the model\n    with torch.no_grad():\n        pred_t = torch.sigmoid(model(img_t.unsqueeze(0).to(device)))\n    pred_t = pred_t.cpu().squeeze(0)                   # [1,H,W]\n    \n    # Convert to numpy uint8 for plotting\n    img_np  = img_t.cpu().numpy().transpose(1,2,0)     # [H,W,3], floats in [0,1]\n    img_np  = (img_np * 255).astype(np.uint8)\n    img_np = img_np[..., ::-1]\n    \n    gt_mask = (mask_t.cpu().numpy().squeeze() > 0.5).astype(np.uint8) * 255\n    pr_mask = (pred_t.cpu().numpy().squeeze() > 0.5).astype(np.uint8) * 255\n    \n    # Make 3-channel versions of the masks\n    gt_rgb = np.stack([gt_mask]*3, axis=-1)\n    pr_rgb = np.stack([pr_mask]*3, axis=-1)\n    \n    # Build a separator and composite image\n    h, w, _ = img_np.shape\n    sep = np.ones((h, 10, 3), dtype=np.uint8) * 128\n    composite = np.concatenate([img_np, sep, gt_rgb, sep, pr_rgb], axis=1)\n    \n    # Plot\n    plt.figure(figsize=(12, 6))\n    plt.axis('off')\n    plt.imshow(composite)\n    plt.title(f\"Sample {idx}: Original | Ground Truth | Prediction\")\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T15:28:36.200790Z","iopub.execute_input":"2025-05-03T15:28:36.201658Z","iopub.status.idle":"2025-05-03T15:28:39.509896Z","shell.execute_reply.started":"2025-05-03T15:28:36.201618Z","shell.execute_reply":"2025-05-03T15:28:39.508884Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **7. Save the model**","metadata":{}},{"cell_type":"code","source":"torch.save(model, \"/kaggle/working/model_weights.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T15:32:47.344924Z","iopub.execute_input":"2025-05-03T15:32:47.345285Z","iopub.status.idle":"2025-05-03T15:32:47.569946Z","shell.execute_reply.started":"2025-05-03T15:32:47.345253Z","shell.execute_reply":"2025-05-03T15:32:47.568922Z"}},"outputs":[],"execution_count":null}]}
